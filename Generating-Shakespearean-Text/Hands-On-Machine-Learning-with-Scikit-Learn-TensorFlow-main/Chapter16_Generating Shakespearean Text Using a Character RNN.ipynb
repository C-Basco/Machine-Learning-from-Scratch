{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "breeding-vegetation",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-iceland",
   "metadata": {},
   "source": [
    "**Project from :\n",
    "Geron, Aurelien.\"Chapter 16: Natural Language Processsing with RNNs and Attention\".*Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*,second edition, O'Reilly Media Inc,2019, 525-534."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-cardiff",
   "metadata": {},
   "source": [
    "A common approach to Neural Language Processing is using recurrent neural networks(RNN). To generate Shakespearean text we will use a Character RNN which is trained to predict the next character in a sentence. \n",
    "\n",
    "We will first build a **stateless RNN**, which learns on random portions of text at each iteration, without any information on the rest of the text. \n",
    "\n",
    "Then we will build a **stateful RNN** , which perserves the hidden state betweeen training iterations and continues reading where it left off. This allows it to learn longer patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-underground",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "rubber-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spanish-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-wales",
   "metadata": {},
   "source": [
    "### print versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accessory-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "young-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-turning",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-humor",
   "metadata": {},
   "source": [
    "Download all of Shakespeare's work, using Keras's `get file()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "known-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" #shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "        shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spiritual-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-jesus",
   "metadata": {},
   "source": [
    "Encode every character as an integer. we will use `keras tokenizer class`.\n",
    "We first fit tokenizer to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abandoned-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "joined-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"]);\n",
    "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "artificial-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # Number of distinct characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "sized-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-christmas",
   "metadata": {},
   "source": [
    "encode full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cellular-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-potter",
   "metadata": {},
   "source": [
    "## How to split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "chemical-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90//100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-denmark",
   "metadata": {},
   "source": [
    "## Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-flashing",
   "metadata": {},
   "source": [
    "We will use the dataset's `window()`. This method will be used to conver the very long sequence of characters into many smaller windows of text.  Currently the training set consists of a single sequence of over a million characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "experienced-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 #target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-pollution",
   "metadata": {},
   "source": [
    "The `window()` method creates a dataset that contains windows, each of which is also represented as a dataset. It's a *nested  dataset*.\n",
    "\n",
    "We can not use a nested dataset directly for training. Our model expects tensor's as input, not datasets.\n",
    "\n",
    "This is where `flat_map()` method comes in.  It converts a nested dataset into a *flat* dataset(one that does not contain datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "exempt-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-conjunction",
   "metadata": {},
   "source": [
    "We call `batch(window_lenghth)` we will get a single tensor for each of them.\n",
    "We need to shuffle the windows then we can batch the windows and seperate the inputs(first 100 characters) from the target(the last character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "comparative-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-animation",
   "metadata": {},
   "source": [
    "categorical input should generally  be encoded, usually as one_hot vectors or as embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "impossible-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-income",
   "metadata": {},
   "source": [
    "need to add prefectching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "normal-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-recording",
   "metadata": {},
   "source": [
    "## Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-culture",
   "metadata": {},
   "source": [
    "**Note:** The GRU class will only use the GPU when using the default values for the following arguments: `activation, recurrent_activation, recurrent_dropout, unroll, use_bias, reset_after`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-signature",
   "metadata": {},
   "source": [
    "*I'll do `epoch=5` only to make ths take less time to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "british-accountability",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31370/31370 [==============================] - 4479s 143ms/step - loss: 1.7185\n",
      "Epoch 2/5\n",
      "31370/31370 [==============================] - 4491s 143ms/step - loss: 1.5433\n",
      "Epoch 3/5\n",
      "31370/31370 [==============================] - 4447s 142ms/step - loss: 1.5189\n",
      "Epoch 4/5\n",
      "31370/31370 [==============================] - 4456s 142ms/step - loss: 1.5054\n",
      "Epoch 5/5\n",
      "31370/31370 [==============================] - 4443s 142ms/step - loss: 1.4960\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                      dropout=0.2),  #recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2),  #recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                   activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-chorus",
   "metadata": {},
   "source": [
    "## Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-virus",
   "metadata": {},
   "source": [
    "We now created a model that can predict next character in text. To feed it text we need to preprocess like earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-pottery",
   "metadata": {},
   "source": [
    "We will create a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "institutional-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-storage",
   "metadata": {},
   "source": [
    "Now use model to predict the next letter in some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "medieval-scott",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"]);\n",
    "Y_pred = model.predict_classes(X_new);\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] #1st sentence, last character 'u'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-owner",
   "metadata": {},
   "source": [
    "## Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-initial",
   "metadata": {},
   "source": [
    "To create more diverse and interesting text ee can we can pick the next character randomly, with a probability equal to the estimated probability, using Tensorflow's tf `tf.random.categorical()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "minute-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) +1 # Here is where\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-proof",
   "metadata": {},
   "source": [
    "This function will repeatedly call `next_char()` to get the next character and append to the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aboriginal-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "honest-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-connecticut",
   "metadata": {},
   "source": [
    "we will now generate some text with different temperatures. We will use *temperatures* to divide logits(clas log probabilities).\n",
    "a temperature closer to 0 will favor high probability characters.\n",
    "< Temperatures will give all characters an equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "married-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ther be a child,\n",
      "and she will be the father to the \n",
      "wath thy partiag;\n",
      "and tell the trurber for you his \n",
      "w's necy\n",
      "housep,\n",
      "upkad w.ti,\n",
      "slwiir kboody?hgtorivo\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=0.2));\n",
    "print(complete_text(\"w\", temperature=1));\n",
    "print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-jurisdiction",
   "metadata": {},
   "source": [
    "# Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-google",
   "metadata": {},
   "source": [
    "Running a stateful RNN will perserve the final state after processing one training batch and use it as the initial state for the next training batch. This allows the model to learn long term patterns. Although it still back propagates through short sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-tiger",
   "metadata": {},
   "source": [
    "The \"batches\" below contain a single window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "geographic-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:,1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "acknowledged-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-conversation",
   "metadata": {},
   "source": [
    "need to set `stateful=True` when creating recurrent layer.\n",
    "Stateful RNN needs to know the batch size so we must set the `batch_input_shape` argument in the first layer.\n",
    "Second dimension unspecified, since inputs could have nay length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "latest-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2, batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                   activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-coffee",
   "metadata": {},
   "source": [
    "At the end of each epoch, we need to rest the states before we go back to the beginning of the text. For this we will use a small callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "assigned-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-kitchen",
   "metadata": {},
   "source": [
    "Compile and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "multiple-amplifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 48s 140ms/step - loss: 2.8961\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 2.2880\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 42s 134ms/step - loss: 2.1400\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 2.0531\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.9960\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.9558\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 45s 145ms/step - loss: 1.9255\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 41s 131ms/step - loss: 1.9023\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 1.8828\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 1.8684\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 1.8527\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.8425\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.8304\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 1.8215\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 1.8116\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 1.8069\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.8015\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.7936\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.7896\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 41s 132ms/step - loss: 1.7848\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.7786\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 1.7744\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 45s 145ms/step - loss: 1.7713\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 45s 145ms/step - loss: 1.7671\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.7616\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 39s 126ms/step - loss: 1.7584\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 40s 128ms/step - loss: 1.7551\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 1.7496\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 43s 136ms/step - loss: 1.7487\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 38s 123ms/step - loss: 1.7477\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 38s 123ms/step - loss: 1.7446\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 1.7406\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 39s 124ms/step - loss: 1.7391\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 39s 124ms/step - loss: 1.7368\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 1.7347\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 39s 126ms/step - loss: 1.7334\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 1.7339\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 39s 124ms/step - loss: 1.7293\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 39s 126ms/step - loss: 1.7293\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 41s 129ms/step - loss: 1.7254\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 1.7242\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 1.7243\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 1.7224\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 44s 142ms/step - loss: 1.7219\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.7192\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.7200\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 42s 134ms/step - loss: 1.7156\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 42s 134ms/step - loss: 1.7150\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 1.7133\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 1.7135\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-jerusalem",
   "metadata": {},
   "source": [
    "To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "honey-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-adams",
   "metadata": {},
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "certified-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "collective-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "infectious-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 156 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A616B6B8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 157 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A616B6B8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "ting's lest,\n",
      "thy censer but a may day doth brued an\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
